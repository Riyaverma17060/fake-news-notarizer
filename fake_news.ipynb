{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d96d946c-b4e2-4908-ac6a-ce12b81d83ac",
   "metadata": {},
   "source": [
    "# Fake News Notarizer: AI + Decentralized Proof"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e0b0db-fd55-4738-9514-175b2ec4f71a",
   "metadata": {},
   "source": [
    "# Setup & Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de52d0be-78a1-4f72-986f-0f8b6d44ac84",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\rohin\\anaconda3\\lib\\site-packages (2.7.1)\n",
      "Requirement already satisfied: datasets in c:\\users\\rohin\\anaconda3\\lib\\site-packages (4.0.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\rohin\\anaconda3\\lib\\site-packages (2.3.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\rohin\\anaconda3\\lib\\site-packages (1.7.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\rohin\\anaconda3\\lib\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\rohin\\anaconda3\\lib\\site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\rohin\\anaconda3\\lib\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\rohin\\anaconda3\\lib\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\rohin\\anaconda3\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\rohin\\anaconda3\\lib\\site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\rohin\\anaconda3\\lib\\site-packages (from torch) (75.1.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\rohin\\anaconda3\\lib\\site-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\rohin\\anaconda3\\lib\\site-packages (from datasets) (16.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\rohin\\anaconda3\\lib\\site-packages (from datasets) (0.3.5.1)\n",
      "Requirement already satisfied: requests>=2.32.2 in c:\\users\\rohin\\anaconda3\\lib\\site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in c:\\users\\rohin\\anaconda3\\lib\\site-packages (from datasets) (4.66.5)\n",
      "Requirement already satisfied: xxhash in c:\\users\\rohin\\anaconda3\\lib\\site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in c:\\users\\rohin\\anaconda3\\lib\\site-packages (from datasets) (0.70.13)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in c:\\users\\rohin\\anaconda3\\lib\\site-packages (from datasets) (0.33.4)\n",
      "Requirement already satisfied: packaging in c:\\users\\rohin\\anaconda3\\lib\\site-packages (from datasets) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\rohin\\anaconda3\\lib\\site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\rohin\\anaconda3\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\rohin\\anaconda3\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\rohin\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: scipy>=1.8.0 in c:\\users\\rohin\\anaconda3\\lib\\site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\rohin\\anaconda3\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\rohin\\anaconda3\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in c:\\users\\rohin\\anaconda3\\lib\\site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.10.5)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\rohin\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\rohin\\anaconda3\\lib\\site-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\rohin\\anaconda3\\lib\\site-packages (from requests>=2.32.2->datasets) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\rohin\\anaconda3\\lib\\site-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\rohin\\anaconda3\\lib\\site-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\rohin\\anaconda3\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\rohin\\anaconda3\\lib\\site-packages (from tqdm>=4.66.3->datasets) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\rohin\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\rohin\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\rohin\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\rohin\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\rohin\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\rohin\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\rohin\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.11.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install torch datasets pandas scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4484464a-640d-4fe0-97b6-650c31d61bc5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: accelerate in c:\\users\\rohin\\anaconda3\\lib\\site-packages (1.9.0)\n",
      "Requirement already satisfied: transformers[torch] in c:\\users\\rohin\\anaconda3\\lib\\site-packages (4.53.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\rohin\\anaconda3\\lib\\site-packages (from transformers[torch]) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in c:\\users\\rohin\\anaconda3\\lib\\site-packages (from transformers[torch]) (0.33.4)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\rohin\\anaconda3\\lib\\site-packages (from transformers[torch]) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\rohin\\anaconda3\\lib\\site-packages (from transformers[torch]) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\rohin\\anaconda3\\lib\\site-packages (from transformers[torch]) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\rohin\\anaconda3\\lib\\site-packages (from transformers[torch]) (2024.9.11)\n",
      "Requirement already satisfied: requests in c:\\users\\rohin\\anaconda3\\lib\\site-packages (from transformers[torch]) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\rohin\\anaconda3\\lib\\site-packages (from transformers[torch]) (0.21.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\rohin\\anaconda3\\lib\\site-packages (from transformers[torch]) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\rohin\\anaconda3\\lib\\site-packages (from transformers[torch]) (4.66.5)\n",
      "Requirement already satisfied: torch>=2.1 in c:\\users\\rohin\\anaconda3\\lib\\site-packages (from transformers[torch]) (2.7.1)\n",
      "Requirement already satisfied: psutil in c:\\users\\rohin\\anaconda3\\lib\\site-packages (from accelerate) (5.9.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\rohin\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.30.0->transformers[torch]) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\rohin\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.30.0->transformers[torch]) (4.11.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\rohin\\anaconda3\\lib\\site-packages (from torch>=2.1->transformers[torch]) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\rohin\\anaconda3\\lib\\site-packages (from torch>=2.1->transformers[torch]) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\rohin\\anaconda3\\lib\\site-packages (from torch>=2.1->transformers[torch]) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\rohin\\anaconda3\\lib\\site-packages (from torch>=2.1->transformers[torch]) (75.1.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\rohin\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers[torch]) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\rohin\\anaconda3\\lib\\site-packages (from requests->transformers[torch]) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\rohin\\anaconda3\\lib\\site-packages (from requests->transformers[torch]) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\rohin\\anaconda3\\lib\\site-packages (from requests->transformers[torch]) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\rohin\\anaconda3\\lib\\site-packages (from requests->transformers[torch]) (2024.8.30)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\rohin\\anaconda3\\lib\\site-packages (from sympy>=1.13.3->torch>=2.1->transformers[torch]) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\rohin\\anaconda3\\lib\\site-packages (from jinja2->torch>=2.1->transformers[torch]) (2.1.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade transformers[torch] accelerate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "297786a3-a69c-4341-b82e-da6f727f9156",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nlpaug in c:\\users\\rohin\\anaconda3\\lib\\site-packages (1.1.11)\n",
      "Requirement already satisfied: nltk in c:\\users\\rohin\\anaconda3\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: numpy>=1.16.2 in c:\\users\\rohin\\anaconda3\\lib\\site-packages (from nlpaug) (1.26.4)\n",
      "Requirement already satisfied: pandas>=1.2.0 in c:\\users\\rohin\\anaconda3\\lib\\site-packages (from nlpaug) (2.3.1)\n",
      "Requirement already satisfied: requests>=2.22.0 in c:\\users\\rohin\\anaconda3\\lib\\site-packages (from nlpaug) (2.32.3)\n",
      "Requirement already satisfied: gdown>=4.0.0 in c:\\users\\rohin\\anaconda3\\lib\\site-packages (from nlpaug) (5.2.0)\n",
      "Requirement already satisfied: click in c:\\users\\rohin\\anaconda3\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\rohin\\anaconda3\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\rohin\\anaconda3\\lib\\site-packages (from nltk) (2024.9.11)\n",
      "Requirement already satisfied: tqdm in c:\\users\\rohin\\anaconda3\\lib\\site-packages (from nltk) (4.66.5)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\rohin\\anaconda3\\lib\\site-packages (from gdown>=4.0.0->nlpaug) (4.12.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\rohin\\anaconda3\\lib\\site-packages (from gdown>=4.0.0->nlpaug) (3.13.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\rohin\\anaconda3\\lib\\site-packages (from pandas>=1.2.0->nlpaug) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\rohin\\anaconda3\\lib\\site-packages (from pandas>=1.2.0->nlpaug) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\rohin\\anaconda3\\lib\\site-packages (from pandas>=1.2.0->nlpaug) (2023.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\rohin\\anaconda3\\lib\\site-packages (from requests>=2.22.0->nlpaug) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\rohin\\anaconda3\\lib\\site-packages (from requests>=2.22.0->nlpaug) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\rohin\\anaconda3\\lib\\site-packages (from requests>=2.22.0->nlpaug) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\rohin\\anaconda3\\lib\\site-packages (from requests>=2.22.0->nlpaug) (2024.8.30)\n",
      "Requirement already satisfied: colorama in c:\\users\\rohin\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\rohin\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas>=1.2.0->nlpaug) (1.16.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\rohin\\anaconda3\\lib\\site-packages (from beautifulsoup4->gdown>=4.0.0->nlpaug) (2.5)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in c:\\users\\rohin\\anaconda3\\lib\\site-packages (from requests[socks]->gdown>=4.0.0->nlpaug) (1.7.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install nlpaug nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d464ca-501b-4d30-ade9-1fb547afb339",
   "metadata": {},
   "source": [
    "# Imports & Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc6865a6-9ecb-4530-bf3e-2d23f103754b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HF_HUB_DISABLE_SYMLINKS_WARNING\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f275eb2-e433-4316-8d0a-f8427027429a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\rohin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\rohin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\rohin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Moon landing place live film in a studio.']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('averaged_perceptron_tagger_eng')  # <— new\n",
    "\n",
    "import nlpaug.augmenter.word as naw\n",
    "aug = naw.SynonymAug(aug_src='wordnet')\n",
    "\n",
    "new_text = aug.augment(\"Moon landing was filmed in a studio.\")\n",
    "print(new_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "597ab6ea-afe1-441d-9b47-0145524b2bfb",
   "metadata": {},
   "source": [
    "# Creating Dataset with Synonym‑Based Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7aa6709a-59a9-4608-a065-d28138e156fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original rows: 10 → Expanded rows: 610\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nlpaug.augmenter.word as naw\n",
    "\n",
    "# Load your original CSV\n",
    "df = pd.read_csv(\"fake_news.csv\")[[\"text\", \"label\"]]\n",
    "\n",
    "# Initialize augmenter\n",
    "aug = naw.SynonymAug(aug_src='wordnet')\n",
    "\n",
    "# Filter only the FAKE examples\n",
    "fake_df = df[df.label == 0]\n",
    "\n",
    "# Generate, say, 5 paraphrases per fake example\n",
    "augmented_rows = []\n",
    "for text in fake_df[\"text\"].tolist():\n",
    "    for i in range(100):  # tweak this count as you like\n",
    "        aug_text = aug.augment(text)\n",
    "\n",
    "        # ✅ Convert list to string if needed\n",
    "        if isinstance(aug_text, list):\n",
    "            aug_text = aug_text[0]\n",
    "\n",
    "        augmented_rows.append({\"text\": aug_text, \"label\": 0})\n",
    "\n",
    "# Build a DataFrame of augmented fakes\n",
    "aug_df = pd.DataFrame(augmented_rows)\n",
    "\n",
    "# Combine with your original data\n",
    "df_expanded = pd.concat([df, aug_df], ignore_index=True)\n",
    "\n",
    "print(\"Original rows:\", len(df), \"→ Expanded rows:\", len(df_expanded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c15775c8-db0a-4129-9957-2eec9c4e521b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fake_news.csv created with 10 samples.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = {\n",
    "    \"text\": [\n",
    "        \"Scientists prove coffee cures Monday blues!\",\n",
    "        \"Moon landing was filmed in a studio.\",\n",
    "        \"New vaccine reduces flu risk by 70% according to study.\",\n",
    "        \"Ancient pyramids discovered on Mars!\",\n",
    "        \"Local bakery introduces zero-calorie croissants.\",\n",
    "        \"Study finds listening to music improves plant growth.\",\n",
    "        \"Government announces free public transport for all citizens.\",\n",
    "        \"Celebrity adopts a pet dragon from a rescue shelter.\",\n",
    "        \"Researchers develop battery that charges in 10 seconds.\",\n",
    "        \"City installs levitating bikes for commuters.\"\n",
    "    ],\n",
    "    \"label\": [0, 0, 1, 0, 0, 1, 1, 0, 1, 0]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv(\"fake_news.csv\", index=False)\n",
    "print(\"fake_news.csv created with 10 samples.\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b54beee7-37cb-4376-9884-8db40ebc560a",
   "metadata": {},
   "source": [
    "# Build & Split Hugging‑Face Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b72fc2a0-f4ee-474b-b5cf-b3bdc98e70da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c50091b33abc4f2c8d1472c42fdf648c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ede571bde56484083c87d4a458a45bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train columns: {'label': Value('int64'), 'input_ids': List(Value('int32')), 'attention_mask': List(Value('int8'))}\n",
      "label           → (8,)\n",
      "input_ids       → (8, 128)\n",
      "attention_mask  → (8, 128)\n"
     ]
    }
   ],
   "source": [
    "# ─── Step 1: Load CSV into a Hugging Face Dataset ─────────────────────────────────\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "# Adjust the path if your CSV is elsewhere\n",
    "df = pd.read_csv(\"fake_news.csv\")[[\"text\", \"label\"]]\n",
    "dataset = Dataset.from_pandas(df)\n",
    "\n",
    "# Split into train / eval\n",
    "splits = dataset.train_test_split(test_size=0.2, seed=42)\n",
    "train_ds = splits[\"train\"]\n",
    "eval_ds  = splits[\"test\"]\n",
    "\n",
    "# ─── Step 2: Tokenize with DistilBERT ───────────────────────────────────────────────\n",
    "from transformers import DistilBertTokenizerFast\n",
    "\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "def tokenize_batch(batch):\n",
    "    return tokenizer(\n",
    "        batch[\"text\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=128\n",
    "    )\n",
    "\n",
    "# Apply tokenization (keeps ‘label’ intact)\n",
    "train_ds = train_ds.map(tokenize_batch, batched=True)\n",
    "eval_ds  = eval_ds.map(tokenize_batch,  batched=True)\n",
    "\n",
    "# ─── Step 3: Remove raw text, set PyTorch format ──────────────────────────────────\n",
    "train_ds = train_ds.remove_columns([\"text\"])\n",
    "eval_ds  = eval_ds.remove_columns([\"text\"])\n",
    "\n",
    "train_ds.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "eval_ds.set_format(\"torch\",  columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "\n",
    "# ─── Step 4: Create DataLoaders ───────────────────────────────────────────────────\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=8, shuffle=True)\n",
    "eval_loader  = DataLoader(eval_ds,  batch_size=16)\n",
    "\n",
    "# ─── Quick Sanity Check ────────────────────────────────────────────────────────────\n",
    "print(\"Train columns:\", train_ds.features)\n",
    "batch = next(iter(train_loader))\n",
    "for name, tensor in batch.items():\n",
    "    print(f\"{name:15s} → {tuple(tensor.shape)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e339d4df-60f0-4cd1-b608-f8284c80c9b2",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d124339f-0df5-4edc-9248-4197e201f2e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 train loss: 0.7547\n",
      "Epoch 1 eval accuracy: 1.00\n",
      "Epoch 2 train loss: 0.6489\n",
      "Epoch 2 eval accuracy: 0.00\n",
      "Epoch 3 train loss: 0.5563\n",
      "Epoch 3 eval accuracy: 0.00\n",
      "Epoch 4 train loss: 0.4798\n",
      "Epoch 4 eval accuracy: 0.00\n",
      "Epoch 5 train loss: 0.4010\n",
      "Epoch 5 eval accuracy: 0.00\n",
      "Epoch 6 train loss: 0.3277\n",
      "Epoch 6 eval accuracy: 0.00\n",
      "Epoch 7 train loss: 0.2500\n",
      "Epoch 7 eval accuracy: 0.00\n",
      "Epoch 8 train loss: 0.1996\n",
      "Epoch 8 eval accuracy: 0.00\n",
      "Epoch 9 train loss: 0.1614\n",
      "Epoch 9 eval accuracy: 0.00\n",
      "Epoch 10 train loss: 0.1210\n",
      "Epoch 10 eval accuracy: 0.00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./model_out\\\\tokenizer_config.json',\n",
       " './model_out\\\\special_tokens_map.json',\n",
       " './model_out\\\\vocab.txt',\n",
       " './model_out\\\\added_tokens.json',\n",
       " './model_out\\\\tokenizer.json')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from transformers import DistilBertForSequenceClassification\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\", num_labels=2\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
    "criterion = CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        input_ids      = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels         = batch[\"label\"].to(device)\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        loss = criterion(outputs.logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1} train loss: {total_loss/len(train_loader):.4f}\")\n",
    "\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for batch in eval_loader:\n",
    "            input_ids      = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels         = batch[\"label\"].to(device)\n",
    "            preds = model(input_ids, attention_mask=attention_mask).logits.argmax(dim=1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total   += labels.size(0)\n",
    "    print(f\"Epoch {epoch+1} eval accuracy: {correct/total:.2f}\")\n",
    "\n",
    "model.save_pretrained(\"./model_out\")\n",
    "tokenizer.save_pretrained(\"./model_out\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b55f26fe-f815-4f2a-9631-36b7f6b0eec9",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "18e9ca99-bd1a-4402-a677-e3c802fa715e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Text     : Study shows that coffee improves memory by 50%.\n",
      "Label    : REAL\n",
      "Confidence: 0.75\n",
      "Text Hash: de3f4ff40a…\n",
      "\n",
      "Text     : Moon landing was filmed in a studio.\n",
      "Label    : FAKE\n",
      "Confidence: 0.93\n",
      "Text Hash: 682f7401f4…\n"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertForSequenceClassification, DistilBertTokenizerFast\n",
    "import torch\n",
    "import hashlib\n",
    "\n",
    "# 1) Load tokenizer + fine‑tuned model\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained(\"./model_out\")\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\"./model_out\")\n",
    "model.eval()\n",
    "\n",
    "# 2) Helper to classify\n",
    "def classify(text):\n",
    "    # Tokenize the input\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=128)\n",
    "    \n",
    "    # Run model in no-grad mode\n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs).logits\n",
    "\n",
    "    # Compute probabilities\n",
    "    probs = torch.softmax(logits, dim=1)[0].tolist()\n",
    "    label = \"FAKE\" if probs[0] > probs[1] else \"REAL\"\n",
    "    confidence = round(max(probs), 2)\n",
    "\n",
    "    # Generate hash for the input text\n",
    "    text_hash = hashlib.sha256(text.encode()).hexdigest()\n",
    "\n",
    "    return {\n",
    "        \"text\": text,\n",
    "        \"label\": label,\n",
    "        \"confidence\": confidence,\n",
    "        \"text_hash\": text_hash\n",
    "    }\n",
    "\n",
    "# 3) Test on your own samples\n",
    "samples = [\n",
    "    \"Study shows that coffee improves memory by 50%.\",\n",
    "    \"Moon landing was filmed in a studio.\"\n",
    "]\n",
    "\n",
    "for sample in samples:\n",
    "    result = classify(sample)\n",
    "    print(f\"\\nText     : {result['text']}\")\n",
    "    print(f\"Label    : {result['label']}\")\n",
    "    print(f\"Confidence: {result['confidence']}\")\n",
    "    print(f\"Text Hash: {result['text_hash'][:10]}…\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "237bafc8-3e99-4ff5-9ebc-918ea62a7f81",
   "metadata": {},
   "source": [
    "# Blockchain‑Style Notarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "895421e9-cd08-4b44-8a31-d4e7fd289111",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block:\n",
    "    def __init__(self, index, timestamp, data, prev_hash):\n",
    "        self.index = index\n",
    "        self.timestamp = timestamp\n",
    "        self.data = data             # e.g. {\"text_hash\":…, \"label\":…, \"conf\":…}\n",
    "        self.prev_hash = prev_hash\n",
    "        self.hash = self.compute_hash()\n",
    "\n",
    "    def compute_hash(self):\n",
    "        block_string = f\"{self.index}{self.timestamp}{self.data}{self.prev_hash}\"\n",
    "        return hashlib.sha256(block_string.encode()).hexdigest()\n",
    "\n",
    "class SimpleChain:\n",
    "    def __init__(self):\n",
    "        # genesis block\n",
    "        genesis = Block(0, time.time(), {\"note\":\"genesis\"}, \"0\")\n",
    "        self.chain = [genesis]\n",
    "\n",
    "    def add_block(self, data):\n",
    "        prev = self.chain[-1]\n",
    "        block = Block(len(self.chain), time.time(), data, prev.hash)\n",
    "        self.chain.append(block)\n",
    "        return block"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5006b97-efec-4297-afbf-ee08cb147303",
   "metadata": {},
   "source": [
    "# Inference Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a7b2029b-7584-48f3-ac4f-d923097e69d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Index</th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>Text_Hash</th>\n",
       "      <th>Label</th>\n",
       "      <th>Conf</th>\n",
       "      <th>Block_Hash</th>\n",
       "      <th>Prev_Hash</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2025-07-20 15:21:11</td>\n",
       "      <td>9f13c2450c…</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>0.93</td>\n",
       "      <td>17a72d4a76…</td>\n",
       "      <td>753bb536ad…</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Index            Timestamp    Text_Hash Label  Conf   Block_Hash  \\\n",
       "0      1  2025-07-20 15:21:11  9f13c2450c…  FAKE  0.93  17a72d4a76…   \n",
       "\n",
       "     Prev_Hash  \n",
       "0  753bb536ad…  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time, hashlib\n",
    "import pandas as pd\n",
    "# Initialize\n",
    "chain = SimpleChain()\n",
    "ledger = []\n",
    "\n",
    "def classify_and_notarize(text):\n",
    "    result = classify(text)  # result is a dict\n",
    "    block = chain.add_block(result)\n",
    "    \n",
    "    ledger.append({\n",
    "        \"Index\": block.index,\n",
    "        \"Timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\", time.gmtime(block.timestamp)),\n",
    "        \"Text_Hash\": result[\"text_hash\"][:10] + \"…\",\n",
    "        \"Label\": result[\"label\"],\n",
    "        \"Conf\": result[\"confidence\"],\n",
    "        \"Block_Hash\": block.hash[:10] + \"…\",\n",
    "        \"Prev_Hash\": block.prev_hash[:10] + \"…\"\n",
    "    })\n",
    "    return ledger[-1]\n",
    "\n",
    "# Try it out\n",
    "df = pd.DataFrame([ classify_and_notarize(\"Scientists prove coffee cures Monday blues!\") ])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d99d27-8610-4aee-a3e9-64ebd4981510",
   "metadata": {},
   "source": [
    "# Final Demo & Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7c451dfc-ed5d-4a3e-9705-5dfd666111de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Index</th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>Text_Hash</th>\n",
       "      <th>Label</th>\n",
       "      <th>Conf</th>\n",
       "      <th>Block_Hash</th>\n",
       "      <th>Prev_Hash</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2025-07-20 15:21:11</td>\n",
       "      <td>9f13c2450c…</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>0.93</td>\n",
       "      <td>17a72d4a76…</td>\n",
       "      <td>753bb536ad…</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2025-07-20 15:21:12</td>\n",
       "      <td>682f7401f4…</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>0.93</td>\n",
       "      <td>344642b4d6…</td>\n",
       "      <td>17a72d4a76…</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Index            Timestamp    Text_Hash Label  Conf   Block_Hash  \\\n",
       "0      1  2025-07-20 15:21:11  9f13c2450c…  FAKE  0.93  17a72d4a76…   \n",
       "1      2  2025-07-20 15:21:12  682f7401f4…  FAKE  0.93  344642b4d6…   \n",
       "\n",
       "     Prev_Hash  \n",
       "0  753bb536ad…  \n",
       "1  17a72d4a76…  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "more = classify_and_notarize(\"Moon landing was filmed in a studio.\")\n",
    "pd.DataFrame(ledger)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
